# Emotion Recognition in Dialogue Agents with an Integrated Memory Module
Imperial Msc Computing Individual Project 
Recent advancements in AI and Natural Language Processing have underscored the need for emotionally intelligent dialogue agents. This repository houses the code and resources pertaining to our efforts to bridge the gap between AI capabilities and emotionally charged human interactions.

Overview
Motivation: The AI community acknowledges the significance of memory management, relational structures in dialogues, commonsense knowledge integration, emotion recognition, and multi-language capabilities. These insights aim to make dialogue agents more relatable and efficient.

Aim: To integratively enhance AI conversational systems using a Memory Module with established large language models. Our work focuses on making these systems adept at classifying human emotions.

Tools & Datasets: Leveraging tools like DialoGPT, OPT, and PyTorch, we've fine-tuned our models on the EmpatheticDialogues dataset to create a dialogue system that continually updates the conversational context based on emotion.

Features
Advanced Memory Module integration with DialoGPT and OPT.
Fine-tuning capabilities using the EmpatheticDialogues dataset.
Custom model training options including learning rate optimization, selective layer freezing, and batch size adjustments.
Enhanced emotion recognition from dialogues, improving the efficacy of AI-human interactions.
Results
Our integrated models outperformed base versions in precision, recall, and F1 score metrics. The integration of Memory Modules in large language models highlights significant improvements in sentiment recognition. This methodology offers expansive applications in NLP, emphasizing its role in advancing sentiment recognition and related tasks.

Getting Started
[To be filled with setup, installation, and usage instructions.]

Future Work
[Optional section highlighting the future directions and enhancements planned for this project.]

License & Acknowledgements
[Information about the license and any acknowledgements to datasets, tools, or references used.]

